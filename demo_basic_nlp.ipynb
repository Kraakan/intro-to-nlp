{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "demo_basic_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kraakan/intro-to-nlp/blob/master/demo_basic_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbdnU7HznK0m"
      },
      "source": [
        "# Basic NLP exercises\n",
        "\n",
        "* During these exercises, you will learn basic Python skills required in NLP, for example\n",
        "  * Reading and processing language data\n",
        "  * Segmenting text\n",
        "  * Calculating word frequencies and idf weights\n",
        "\n",
        "* Exercises are based on tweets downloaded using Twitter API. Both Finnish and English tweets are available, you are free to choose which language you want to work with.\n",
        "\n",
        "\n",
        "> Finnish: http://dl.turkunlp.org/intro-to-nlp/finnish-tweets-sample.jsonl.gz\n",
        "\n",
        "> English: http://dl.turkunlp.org/intro-to-nlp/english-tweets-sample.jsonl.gz\n",
        "\n",
        "\n",
        "* Both files include 10,000 tweets. If processing the whole file takes too much time, you can also read just a subset of the data, for example only 1,000 tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmlbS9GLm596"
      },
      "source": [
        "## 1) Read tweets in Python\n",
        "\n",
        "* Download the file, and read the data in Python\n",
        "* **The outcome of this exercise** should be a list of tweets, where each tweet is a dictionary including different (key, value) pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmv7kZlI9zCU",
        "outputId": "250f3249-438b-4dba-e612-63296318bb33"
      },
      "source": [
        "!wget -nc http://dl.turkunlp.org/intro-to-nlp/finnish-tweets-sample.jsonl.gz\r\n",
        "#How to unzip?\r\n",
        "import gzip\r\n",
        "import json\r\n",
        "tweets = []\r\n",
        "#Iterate over jsonl and call json.load for each\r\n",
        "f = gzip.open(\"finnish-tweets-sample.jsonl.gz\", \"rt\", encoding=\"utf-8\")\r\n",
        "lines = f.readlines()[:1000]\r\n",
        "for line in lines:\r\n",
        "    data = json.loads(line)\r\n",
        "    tweets.append(data)\r\n",
        "print(tweets[0].keys())\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘finnish-tweets-sample.jsonl.gz’ already there; not retrieving.\n",
            "\n",
            "dict_keys(['retweeted_status', 'retweet_count', 'favorited', 'geo', 'is_quote_status', 'in_reply_to_user_id', 'place', 'id', 'timestamp_ms', 'coordinates', 'truncated', 'id_str', 'in_reply_to_status_id', 'source', 'in_reply_to_user_id_str', 'text', 'in_reply_to_screen_name', 'contributors', 'retweeted', 'lang', 'created_at', 'filter_level', 'in_reply_to_status_id_str', 'favorite_count', 'entities', 'user'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8mHlc_9m5-H"
      },
      "source": [
        "## 2) Extract texts from the tweet jsons\n",
        "\n",
        "* During these exercises we need only the actual tweet text. Inspect the dictionary and extract the actual text field for each tweet.\n",
        "* When carefully inspecting the dictionary keys and values, you may see the old Twitter character limit causing unexpect behavior for text. In these cases, are you able to extract the full text?\n",
        "* **The outcome of this exercise** should be a list of tweets, where each tweet is a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1hCGcbcm5-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9697f6d5-1f11-43bb-ed61-aafc1bb9cc03"
      },
      "source": [
        "tweetlist = []\r\n",
        "for i in tweets:\r\n",
        "    tweetlist.append(i[\"text\"])\r\n",
        "print(len(tweetlist))\r\n",
        "print(tweetlist[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "RT @TommiKutilainen: Ranskassa lakiehdotusessa mukana OA ja avoin data, mm oikeudesta rinnakkaisjulkaisemiseen kustantajasta riippumatta  h…\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04OtJSi8m5-O"
      },
      "source": [
        "## 3) Segment tweets\n",
        "\n",
        "* Segment tweets using the UDPipe machine learned model, remember to select the correct language.\n",
        "\n",
        "> English model: https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/en.segmenter.udpipe\n",
        "\n",
        "> Finnish model: https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/fi.segmenter.udpipe\n",
        "\n",
        "* Note that the segmentation model was not trained on tweets, so it may have difficulties in some cases. Inspect the output to get an idea how well it performs on tweets.\n",
        "* Note: In case of the notebook cell dies while trying to load/run the model, the most typical reason is wrong file path or name, or incorrectly downloaded model.\n",
        "* **The output of this excercise** should be a list of segmented tweets, where each tweet is a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnzl3edh2TEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355c31c0-7f63-4fc0-befb-8243fa71faba"
      },
      "source": [
        "!wget -nc https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/fi.segmenter.udpipe\r\n",
        "!pip3 install ufal.udpipe\r\n",
        "import ufal.udpipe as segmenter"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘fi.segmenter.udpipe’ already there; not retrieving.\n",
            "\n",
            "Collecting ufal.udpipe\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 4.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625274 sha256=9932a569e95896f60c1a4e58a2182a77ab6bf94225f4f39cfa2dd4ae2e7ebfa6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe\n",
            "Successfully installed ufal.udpipe-1.2.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rAwzwVy2V8S"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY-tw8nj2WUr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgFEubVym5-S"
      },
      "source": [
        "## 4) Calculate word frequencies\n",
        "\n",
        "* Calculate a word frequency list (how many times each word appears) based on the tweets. Which are the most common words appearing in the data?\n",
        "* Calculate the size of your vocabulary (how many unique words there are).\n",
        "* **The output of this excercise** should be a sorted list of X most common words and their frequencies, and the number of unique words in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L-qLoLum5-S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaC9e4Rjm5-T"
      },
      "source": [
        "## 5) Calculate idf weights\n",
        "\n",
        "* Calculate idf weight for each word appearing in the data (one tweet = one document), and print top 20 words with lowest and highest idf values.\n",
        "* Can you think of a reason why someone could claim that tf does not have a high impact when processing tweets?\n",
        "* **The output of this excercise** should be a list of words sorted by their idf weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pjNY3H9m5-U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwxJM9Rdm5-U"
      },
      "source": [
        "## 6) Duplicates or near duplicates\n",
        "\n",
        "* Check whether we have duplicate tweets (in terms of text field only) in our dataset. Duplicate tweet means here that the exactly same tweet text appears more than once in our dataset.\n",
        "* Note: It makes sense to check the duplicates using original tweet texts as the texts were before segmentation. I would also recommend using the full 10,000 dataset here in order to get higher chance of seeing duplicates (this does not require heavy computing).\n",
        "* Try to check whether tweets have additional near-duplicates. Near duplicate means here that tweet text is almost the same in two or more tweets. Ponder what kind of near duplicates there could be and how to find those. Start by considering for example different normalization techniques. Implement some of the techniques you considered.\n",
        "* **The outcome of this exercise** should be a number of unique tweets in our dataset (with possibly counting also which are the most common duplicates) as well as the number of unique tweets after removing also near duplicates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jt0sZrSm5-V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}