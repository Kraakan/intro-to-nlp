{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "demo_basic_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kraakan/intro-to-nlp/blob/master/demo_basic_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbdnU7HznK0m"
      },
      "source": [
        "# Basic NLP exercises\n",
        "\n",
        "* During these exercises, you will learn basic Python skills required in NLP, for example\n",
        "  * Reading and processing language data\n",
        "  * Segmenting text\n",
        "  * Calculating word frequencies and idf weights\n",
        "\n",
        "* Exercises are based on tweets downloaded using Twitter API. Both Finnish and English tweets are available, you are free to choose which language you want to work with.\n",
        "\n",
        "\n",
        "> Finnish: http://dl.turkunlp.org/intro-to-nlp/finnish-tweets-sample.jsonl.gz\n",
        "\n",
        "> English: http://dl.turkunlp.org/intro-to-nlp/english-tweets-sample.jsonl.gz\n",
        "\n",
        "\n",
        "* Both files include 10,000 tweets. If processing the whole file takes too much time, you can also read just a subset of the data, for example only 1,000 tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmlbS9GLm596"
      },
      "source": [
        "## 1) Read tweets in Python\n",
        "\n",
        "* Download the file, and read the data in Python\n",
        "* **The outcome of this exercise** should be a list of tweets, where each tweet is a dictionary including different (key, value) pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmv7kZlI9zCU",
        "outputId": "811b89cb-6f88-45fc-fafb-f17f68c4cf76"
      },
      "source": [
        "# When I opened this .ipynb in colab a lot of my changes were gone, but I can still see them on github\r\n",
        "!wget -nc http://dl.turkunlp.org/intro-to-nlp/finnish-tweets-sample.jsonl.gz\r\n",
        "# How to unzip?\r\n",
        "import gzip\r\n",
        "import json\r\n",
        "tweets = []\r\n",
        "# Iterate over jsonl and call json.load for each\r\n",
        "f = gzip.open(\"finnish-tweets-sample.jsonl.gz\", \"rt\", encoding=\"utf-8\")\r\n",
        "lines = f.readlines()\r\n",
        "for line in lines:\r\n",
        "    data = json.loads(line)\r\n",
        "    tweets.append(data)\r\n",
        "print(\"Here are the json keys: \", tweets[0].keys())\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-16 03:58:43--  http://dl.turkunlp.org/intro-to-nlp/finnish-tweets-sample.jsonl.gz\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6120485 (5.8M) [application/octet-stream]\n",
            "Saving to: ‚Äòfinnish-tweets-sample.jsonl.gz‚Äô\n",
            "\n",
            "finnish-tweets-samp 100%[===================>]   5.84M  4.88MB/s    in 1.2s    \n",
            "\n",
            "2021-01-16 03:58:44 (4.88 MB/s) - ‚Äòfinnish-tweets-sample.jsonl.gz‚Äô saved [6120485/6120485]\n",
            "\n",
            "dict_keys(['retweeted_status', 'retweet_count', 'favorited', 'geo', 'is_quote_status', 'in_reply_to_user_id', 'place', 'id', 'timestamp_ms', 'coordinates', 'truncated', 'id_str', 'in_reply_to_status_id', 'source', 'in_reply_to_user_id_str', 'text', 'in_reply_to_screen_name', 'contributors', 'retweeted', 'lang', 'created_at', 'filter_level', 'in_reply_to_status_id_str', 'favorite_count', 'entities', 'user'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8mHlc_9m5-H"
      },
      "source": [
        "## 2) Extract texts from the tweet jsons\n",
        "\n",
        "* During these exercises we need only the actual tweet text. Inspect the dictionary and extract the actual text field for each tweet.\n",
        "* When carefully inspecting the dictionary keys and values, you may see the old Twitter character limit causing unexpect behavior for text. In these cases, are you able to extract the full text?\n",
        "* **The outcome of this exercise** should be a list of tweets, where each tweet is a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1hCGcbcm5-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa84b2e-e735-4ad7-d9f3-84e41c529c27"
      },
      "source": [
        "from random import *\r\n",
        "tweetlist = []\r\n",
        "# Improve: 1. Check if tweet is truncated\r\n",
        "#          2. Add full tweet\r\n",
        "for i in tweets:\r\n",
        "    tweetlist.append(i[\"text\"])\r\n",
        "print(\"Total number of tweets: \", len(tweetlist))\r\n",
        "print(\"Random example: \", tweetlist[randint(0,len(tweetlist))])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of tweets:  10000\n",
            "Random example:  @JanneRautakoski @ElinaNiiranen_ Turvallisia ajokilometrej√§! üôÇ üåø\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04OtJSi8m5-O"
      },
      "source": [
        "## 3) Segment tweets\n",
        "\n",
        "* Segment tweets using the UDPipe machine learned model, remember to select the correct language.\n",
        "\n",
        "> English model: https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/en.segmenter.udpipe\n",
        "\n",
        "> Finnish model: https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/fi.segmenter.udpipe\n",
        "\n",
        "* Note that the segmentation model was not trained on tweets, so it may have difficulties in some cases. Inspect the output to get an idea how well it performs on tweets.\n",
        "* Note: In case of the notebook cell dies while trying to load/run the model, the most typical reason is wrong file path or name, or incorrectly downloaded model.\n",
        "* **The output of this excercise** should be a list of segmented tweets, where each tweet is a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnzl3edh2TEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae018cdb-4005-4be5-c14f-0a11e75df4b8"
      },
      "source": [
        "!wget -nc https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/fi.segmenter.udpipe\r\n",
        "!pip3 install ufal.udpipe\r\n",
        "import ufal.udpipe as udpipe\r\n",
        "\r\n",
        "model = udpipe.Model.load(\"fi.segmenter.udpipe\")\r\n",
        "pipeline = udpipe.Pipeline(model,\"tokenize\",\"none\",\"none\",\"horizontal\") # horizontal: returns one sentence per line, with words separated by a single space\r\n",
        "segmented_tweets=[]\r\n",
        "for t in tweetlist:\r\n",
        "    segmented_tweets.append(pipeline.process(t))\r\n",
        "print(\"Random example: \", segmented_tweets[randint(0,len(segmented_tweets))])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‚Äòfi.segmenter.udpipe‚Äô already there; not retrieving.\n",
            "\n",
            "Requirement already satisfied: ufal.udpipe in /usr/local/lib/python3.6/dist-packages (1.2.0.3)\n",
            "Random example:  @rapvoid kukira mimpiin mitonüòîüò†\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgFEubVym5-S"
      },
      "source": [
        "## 4) Calculate word frequencies\n",
        "\n",
        "* Calculate a word frequency list (how many times each word appears) based on the tweets. Which are the most common words appearing in the data?\n",
        "* Calculate the size of your vocabulary (how many unique words there are).\n",
        "* **The output of this excercise** should be a sorted list of X most common words and their frequencies, and the number of unique words in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L-qLoLum5-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c47806-a6aa-432f-8ceb-d3b236cd0e10"
      },
      "source": [
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "token_counter = Counter()\r\n",
        "for s in segmented_tweets:\r\n",
        "    tokenized = pipeline.process(s)\r\n",
        "    tokens = tokenized.split() # after segmenter, we can do whitespace splitting\r\n",
        "    token_counter.update(tokens)\r\n",
        "\r\n",
        "print(\"Most common tokens:\", token_counter.most_common(20))\r\n",
        "print(\"Vocabulary size:\", len(token_counter))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common tokens: [('.', 5162), ('‚Ä¶', 4281), (',', 4088), (':', 3932), ('#', 3543), ('RT', 2766), ('ja', 2482), ('on', 2243), ('!', 1718), ('?', 1092), ('\"', 925), ('ei', 820), ('ett√§', 690), ('-', 499), ('(', 494), ('‚Äù', 439), (')', 437), ('se', 417), ('‚Äì', 387), ('kun', 343)]\n",
            "Vocabulary size: 53204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaC9e4Rjm5-T"
      },
      "source": [
        "## 5) Calculate idf weights\n",
        "\n",
        "* Calculate idf weight for each word appearing in the data (one tweet = one document), and print top 20 words with lowest and highest idf values.\n",
        "* Can you think of a reason why someone could claim that tf does not have a high impact when processing tweets?\n",
        "* **The output of this excercise** should be a list of words sorted by their idf weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pjNY3H9m5-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f9562b-46af-436c-ced0-4a3be4ed1747"
      },
      "source": [
        "# DF = document frequency df(t), in how many documents (out of all documents) the term t appears\r\n",
        "# IDF = inverse document frequency, m/df(t), where m is the total number of documents in your collection\r\n",
        "DF = {}\r\n",
        "IDF = {}\r\n",
        "import random\r\n",
        "example_token=random.choice(list(token_counter.keys()))\r\n",
        "print(\"Total count for '\", example_token,\"' (selected at random): \", token_counter[example_token])\r\n",
        "print(\"The next part of my code is slow, but it gets there eventually!\")\r\n",
        "for t in token_counter.keys():\r\n",
        "  for s in segmented_tweets:\r\n",
        "      if t in s:\r\n",
        "        if t in DF:\r\n",
        "           DF[t]+=1\r\n",
        "        else: DF[t]=1\r\n",
        "print(\"Document frequency for '\", example_token,\"': \", DF[example_token])\r\n",
        "for t in DF:\r\n",
        "  IDF[t]=len(segmented_tweets)/DF[t]\r\n",
        "print(\"Inverse document frequency for '\", example_token,\"': \", IDF[example_token])  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total count for ' https://t.co/sKlRZGxzmB ' (selected at random):  1\n",
            "The next part of my code is slow, but it gets there eventually!\n",
            "Document frequency for ' https://t.co/sKlRZGxzmB ':  1\n",
            "Inverse document frequency for ' https://t.co/sKlRZGxzmB ':  10000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwxJM9Rdm5-U"
      },
      "source": [
        "## 6) Duplicates or near duplicates\n",
        "\n",
        "* Check whether we have duplicate tweets (in terms of text field only) in our dataset. Duplicate tweet means here that the exactly same tweet text appears more than once in our dataset.\n",
        "* Note: It makes sense to check the duplicates using original tweet texts as the texts were before segmentation. I would also recommend using the full 10,000 dataset here in order to get higher chance of seeing duplicates (this does not require heavy computing).\n",
        "* Try to check whether tweets have additional near-duplicates. Near duplicate means here that tweet text is almost the same in two or more tweets. Ponder what kind of near duplicates there could be and how to find those. Start by considering for example different normalization techniques. Implement some of the techniques you considered.\n",
        "* **The outcome of this exercise** should be a number of unique tweets in our dataset (with possibly counting also which are the most common duplicates) as well as the number of unique tweets after removing also near duplicates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jt0sZrSm5-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0252e6aa-75bf-41fa-a9af-d5ba7da12050"
      },
      "source": [
        "dupe_counter = Counter()\r\n",
        "dupe_counter.update(tweetlist)\r\n",
        "print(\"Most common tweets:\", dupe_counter.most_common(20))\r\n",
        "uniquetweets=[]\r\n",
        "for t in dupe_counter:\r\n",
        "  if dupe_counter[t]==1: uniquetweets.append(t)\r\n",
        "print(\"Number of unique tweets: \", len(uniquetweets))\r\n",
        "print(\"Random example: \", uniquetweets[randint(0,len(uniquetweets))])\r\n",
        "# The duplicates seem to be mostly retweets, so I'm thinking near duplicates might be retweets with something added\r\n",
        "# - though I've ever tweeted myself so I'm not sure...\r\n",
        "neardupe_counter = {}\r\n",
        "for t in uniquetweets:\r\n",
        "  for s in uniquetweets:\r\n",
        "    if t in s:\r\n",
        "      if t in neardupe_counter: neardupe_counter[t]+=1\r\n",
        "      else:neardupe_counter[t]=1\r\n",
        "print(\"For some reason the next part appears only after a short delay.\")\r\n",
        "superuniquetweets=[]\r\n",
        "for t in neardupe_counter:\r\n",
        "  if neardupe_counter[t]==1: superuniquetweets.append(t)\r\n",
        "print(\"Number of super unique tweets: \", len(superuniquetweets))\r\n",
        "print(\"Random example: \", superuniquetweets[randint(0,len(superuniquetweets))])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common tweets: [('RT @SitaSalminen: Testasin huvikseen millaisen reaktion lempeys saa aikaan. Suu loksahti auki https://t.co/L7RR70QsZo', 9), ('RT @KeyisQueen: Sosa babyy https://t.co/raoAJv8auH', 8), ('RT @babaBC: T‚Äôchalla jata hoon kisi ki dhun mein https://t.co/oRkEoGg15B', 8), ('RT @AestheticsJapan: Guiding Light | by Julius K√§hk√∂nen\\n(https://t.co/vZMCQYy8Rt) https://t.co/XbnOyg8l7w', 8), ('RT @alvaleryae: Que hermoooosuuraa https://t.co/Tvha0K9GQC', 5), ('RT @BTS_army_Fin: [COMEBACK GOALS FINLAND!]‚ö†Ô∏è\\n\\nT√§ss√§ @BTS_twt Comeback tavoitteet Suomessa.\\n\\nN√§it√§ ei ole helppo saavuttaa, meid√§n pit√§√§ ol‚Ä¶', 4), ('RT @HelsinkiKymp: Tiedoksi:\\nHelsingin puistoissa ja yleisill√§ alueilla saa liikkua my√∂s \"ilman j√§rkev√§n tuntuista syyt√§\". \\n\\nTuollainen p√§√§m‚Ä¶', 4), ('RT @VartiainenPasi: N√§ytt√§k√§√§ minulle ennustaja, joka tiesi tulevaksi n√§m√§ otsikot vuosi sitten. https://t.co/apGqjDFRWY', 4), ('RT @nastynapalm: Mood ku CV:s ei lue mit√§√§n muuta ku nimi, yhteystiedot sek√§ \"Puhe- ja kirjoitustaito\" ja ty√∂paikkojen ovet vaan pamahtelee‚Ä¶', 4), ('RT @qualityjaehyun: Jaehyun locksreensüíå https://t.co/TompDxLokd', 3), ('RT @seppokoira: Uskomaton kasvutarina! ‚òÄÔ∏è\\n\\nT√§n√§√§n oli niin aurinkoista, ett√§ hukkasin yhden mulle heitetyn juustonpalan valoisaan keitti√∂nn‚Ä¶', 3), ('RT @AnttiHeikkinen: ABC on edelleen luotetuin huoltoasema - suuri kiitos asiakkaat ja teit√§ ilolla palvelevat ABC-ammattilaiset ymp√§ri Suom‚Ä¶', 3), ('RT @lalpra: Sarkara Badalisi, BJP Gellisi https://t.co/CxN08L9NXF', 3), ('RT @armanalizad: HYV√Ñ @saaraaalto MONSTERS ON MUN T√ÑN HETKEN LEMPIBIISI NYT T√Ñ√Ñ TWIITTI JAKOON NIIN ETT√Ñ TWITTER R√ÑJ√ÑHT√Ñ√Ñ JA SAARA VOITTAA‚Ä¶', 3), ('RT @sarasalomaa: \"Kaikilla meill√§ on rajoitteita.\"\\nT√§m√§ lause j√§√§ el√§m√§√§n. Sanat k√∂yh√§lle, sairaalle, ty√∂kyvytt√∂m√§lle ihmiselle. \\n#yleastud‚Ä¶', 3), ('RT @Kaisa_v: \"Jos sotalapsiin olisi sovellettu nykyisi√§ lakeja ja k√§yt√§nt√∂j√§, yksik√§√§n heist√§ ei olisi palannut Suomeen, koska perhesiteen‚Ä¶', 3), ('RT @topielomaa: Kev√§√§n p√∂risev√§t pisti√§iset haltuun. Muista n√§m√§ ennen kuin teet h√§tik√∂ityj√§ liikkeit√§. #kev√§t #kev√§tseuranta https://t.co/‚Ä¶', 3), ('RT @liandersson: Tiesitk√∂ sin√§, ett√§ eduskunta t√§n√§√§n hyv√§ksyi CETA-sopimuksen, tai ett√§ hallituksen tajuton sote-aikataulu on johtanut sii‚Ä¶', 3), ('RT @VilleNiinisto: Kaikkien p√§√§tt√§jien on kyett√§v√§ tunnistamaan poliittisen v√§kivallan vaarallinen erityisluonne: tavoite kumota demokratia‚Ä¶', 2), ('RT @SaucierSuellen: Hotellit kohteessa Keski-Suomi ‚Äì Etsi KAYAK-sivustolta https://t.co/4d7TM2ARgz', 2)]\n",
            "Number of unique tweets:  9633\n",
            "Random example:  RT @KarvalaKreeta: Sipil√§ soten valinnanvapauden nollas√§√§st√∂st√§: ‚ÄùEi sit√§ tiet√§√§kseni ole koskaan perusteltu s√§√§st√∂ill√§‚Äù #sote \n",
            " https://t.‚Ä¶\n",
            "For some reason the next part appears only after a short delay.\n",
            "Number of super unique tweets:  9626\n",
            "Random example:  @SwanOfTuonela Interrail on loppuk√§ytt√§j√§lleen ilmainen, samalla tapaa kuin vaikka suomalainen peruskoulu.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqR2YGRBFddJ"
      },
      "source": [
        "Comments:\r\n",
        "I'm very new to python, but I managed to get by with liberal copy-pasting and some googling.\r\n",
        "I never managed to find the untruncated tweets though."
      ]
    }
  ]
}