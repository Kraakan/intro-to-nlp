{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "demo_basic_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kraakan/intro-to-nlp/blob/master/demo_basic_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbdnU7HznK0m"
      },
      "source": [
        "# Basic NLP exercises\n",
        "\n",
        "* During these exercises, you will learn basic Python skills required in NLP, for example\n",
        "  * Reading and processing language data\n",
        "  * Segmenting text\n",
        "  * Calculating word frequencies and idf weights\n",
        "\n",
        "* Exercises are based on tweets downloaded using Twitter API. Both Finnish and English tweets are available, you are free to choose which language you want to work with.\n",
        "\n",
        "\n",
        "> Finnish: http://dl.turkunlp.org/intro-to-nlp/finnish-tweets-sample.jsonl.gz\n",
        "\n",
        "> English: http://dl.turkunlp.org/intro-to-nlp/english-tweets-sample.jsonl.gz\n",
        "\n",
        "\n",
        "* Both files include 10,000 tweets. If processing the whole file takes too much time, you can also read just a subset of the data, for example only 1,000 tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmlbS9GLm596"
      },
      "source": [
        "## 1) Read tweets in Python\n",
        "\n",
        "* Download the file, and read the data in Python\n",
        "* **The outcome of this exercise** should be a list of tweets, where each tweet is a dictionary including different (key, value) pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmv7kZlI9zCU",
        "outputId": "6c69f12a-5087-436e-a6e6-4f11525409b8"
      },
      "source": [
        "# When I opened this .ipynb in colab a lot of my changes were gone, but I can still see them on github\r\n",
        "!wget -nc http://dl.turkunlp.org/intro-to-nlp/finnish-tweets-sample.jsonl.gz\r\n",
        "# How to unzip?\r\n",
        "import gzip\r\n",
        "import json\r\n",
        "tweets = []\r\n",
        "# Iterate over jsonl and call json.load for each\r\n",
        "f = gzip.open(\"finnish-tweets-sample.jsonl.gz\", \"rt\", encoding=\"utf-8\")\r\n",
        "lines = f.readlines()\r\n",
        "for line in lines:\r\n",
        "    data = json.loads(line)\r\n",
        "    tweets.append(data)\r\n",
        "print(\"Here are the json keys: \", tweets[0].keys())\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-25 06:47:48--  http://dl.turkunlp.org/intro-to-nlp/finnish-tweets-sample.jsonl.gz\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6120485 (5.8M) [application/octet-stream]\n",
            "Saving to: ‘finnish-tweets-sample.jsonl.gz’\n",
            "\n",
            "finnish-tweets-samp 100%[===================>]   5.84M  5.16MB/s    in 1.1s    \n",
            "\n",
            "2021-01-25 06:47:49 (5.16 MB/s) - ‘finnish-tweets-sample.jsonl.gz’ saved [6120485/6120485]\n",
            "\n",
            "Here are the json keys:  dict_keys(['retweeted_status', 'retweet_count', 'favorited', 'geo', 'is_quote_status', 'in_reply_to_user_id', 'place', 'id', 'timestamp_ms', 'coordinates', 'truncated', 'id_str', 'in_reply_to_status_id', 'source', 'in_reply_to_user_id_str', 'text', 'in_reply_to_screen_name', 'contributors', 'retweeted', 'lang', 'created_at', 'filter_level', 'in_reply_to_status_id_str', 'favorite_count', 'entities', 'user'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8mHlc_9m5-H"
      },
      "source": [
        "## 2) Extract texts from the tweet jsons\n",
        "\n",
        "* During these exercises we need only the actual tweet text. Inspect the dictionary and extract the actual text field for each tweet.\n",
        "* When carefully inspecting the dictionary keys and values, you may see the old Twitter character limit causing unexpect behavior for text. In these cases, are you able to extract the full text?\n",
        "* **The outcome of this exercise** should be a list of tweets, where each tweet is a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1hCGcbcm5-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b2a669-7151-416c-eec8-8d223b505bcc"
      },
      "source": [
        "from random import *\r\n",
        "tweetlist = []\r\n",
        "torsoja = []\r\n",
        "# Improve: 1. Check if tweet is truncated\r\n",
        "#          2. Add full tweet\r\n",
        "\r\n",
        "# Found this on stackoverflow:\r\n",
        "def finditem(obj, key):\r\n",
        "  if key in obj: return obj[key]\r\n",
        "  for k, v in obj.items():\r\n",
        "    if isinstance(v,dict):\r\n",
        "      item=finditem(v,key)\r\n",
        "      if item is not None:\r\n",
        "        return item\r\n",
        "  return False\r\n",
        "\r\n",
        "for i in tweets:\r\n",
        "    # Check for longer text\r\n",
        "    t=finditem(i, \"full_text\")\r\n",
        "    if t: tweetlist.append(t)\r\n",
        "    else:\r\n",
        "      tweetlist.append(i['text'])\r\n",
        "      if '…' in i['text'] and len(i['text'])>120: torsoja.append(i['text'])\r\n",
        "\r\n",
        "print(\"Total number of tweets: \", len(tweetlist))\r\n",
        "r = randint(0,len(torsoja))\r\n",
        "if len(torsoja)>0:\r\n",
        "  print(\"Truncated tweets left in material: \", len(torsoja))\r\n",
        "  print(\"Random example nr. \", r, \": \", torsoja[r])\r\n",
        "else:\r\n",
        "  r = randint(0,len(tweetlist))\r\n",
        "  print(\"Random example nr. \", r, \": \", tweetlist[r])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of tweets:  10000\n",
            "Truncated tweets left in material:  336\n",
            "Random example nr.  47 :  RT @MattiLievonen: Tulevaisuutta pitää katsoa avarasti: ei joko-tai vaan sekä-että. Ajan itsekin hybridillä #biopolttoaineet #sähköauto htt…\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_eOo8rZVohJ"
      },
      "source": [
        "Not sure why 336 truncated tweets managed to get through...\r\n",
        "I checked a random example (torsoja[47]), and it didn't seem to have the full text at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04OtJSi8m5-O"
      },
      "source": [
        "## 3) Segment tweets\n",
        "\n",
        "* Segment tweets using the UDPipe machine learned model, remember to select the correct language.\n",
        "\n",
        "> English model: https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/en.segmenter.udpipe\n",
        "\n",
        "> Finnish model: https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/fi.segmenter.udpipe\n",
        "\n",
        "* Note that the segmentation model was not trained on tweets, so it may have difficulties in some cases. Inspect the output to get an idea how well it performs on tweets.\n",
        "* Note: In case of the notebook cell dies while trying to load/run the model, the most typical reason is wrong file path or name, or incorrectly downloaded model.\n",
        "* **The output of this excercise** should be a list of segmented tweets, where each tweet is a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnzl3edh2TEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd72e2d-767c-4ef0-ee40-1836d8f0c204"
      },
      "source": [
        "!wget -nc https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/fi.segmenter.udpipe\r\n",
        "!pip3 install ufal.udpipe\r\n",
        "import ufal.udpipe as udpipe\r\n",
        "\r\n",
        "model = udpipe.Model.load(\"fi.segmenter.udpipe\")\r\n",
        "pipeline = udpipe.Pipeline(model,\"tokenize\",\"none\",\"none\",\"horizontal\") # horizontal: returns one sentence per line, with words separated by a single space\r\n",
        "segmented_tweets=[]\r\n",
        "for t in tweetlist:\r\n",
        "    segmented_tweets.append(pipeline.process(t))\r\n",
        "print(\"Random example: \", segmented_tweets[randint(0,len(segmented_tweets))])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-25 07:03:57--  https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/fi.segmenter.udpipe\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/TurkuNLP/intro-to-nlp/master/Data/fi.segmenter.udpipe [following]\n",
            "--2021-01-25 07:03:57--  https://raw.githubusercontent.com/TurkuNLP/intro-to-nlp/master/Data/fi.segmenter.udpipe\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22607316 (22M) [application/octet-stream]\n",
            "Saving to: ‘fi.segmenter.udpipe’\n",
            "\n",
            "fi.segmenter.udpipe 100%[===================>]  21.56M  45.3MB/s    in 0.5s    \n",
            "\n",
            "2021-01-25 07:03:58 (45.3 MB/s) - ‘fi.segmenter.udpipe’ saved [22607316/22607316]\n",
            "\n",
            "Collecting ufal.udpipe\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 4.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625266 sha256=5e5cff95e32d236632f741bbd77f606669b7a8a6f6e734f5b81ab7e0d64de02a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe\n",
            "Successfully installed ufal.udpipe-1.2.0.3\n",
            "Random example:  Nyt muuten alkaa olemaan sen verta vihreää , että voi sanoa olevan kesä , toukokuun puolivälissä .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgFEubVym5-S"
      },
      "source": [
        "## 4) Calculate word frequencies\n",
        "\n",
        "* Calculate a word frequency list (how many times each word appears) based on the tweets. Which are the most common words appearing in the data?\n",
        "* Calculate the size of your vocabulary (how many unique words there are).\n",
        "* **The output of this excercise** should be a sorted list of X most common words and their frequencies, and the number of unique words in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L-qLoLum5-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b02afb60-c703-4be2-a6dc-5e944b053642"
      },
      "source": [
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "token_counter = Counter()\r\n",
        "for s in segmented_tweets:\r\n",
        "    tokenized = pipeline.process(s)\r\n",
        "    tokens = tokenized.split() # after segmenter, we can do whitespace splitting\r\n",
        "    token_counter.update(tokens)\r\n",
        "\r\n",
        "print(\"20 most common tokens:\", token_counter.most_common(20))\r\n",
        "print(\"Vocabulary size:\", len(token_counter))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 most common tokens: [('.', 8998), ('#', 6836), (',', 5916), ('ja', 3675), ('on', 3029), (':', 2544), ('!', 2285), ('?', 1528), ('\"', 1339), ('ei', 1216), ('RT', 1074), ('että', 969), ('-', 660), ('”', 651), (')', 637), ('(', 619), ('se', 595), ('kun', 478), ('ole', 453), ('mutta', 444)]\n",
            "Vocabulary size: 64984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaC9e4Rjm5-T"
      },
      "source": [
        "## 5) Calculate idf weights\n",
        "\n",
        "* Calculate idf weight for each word appearing in the data (one tweet = one document), and print top 20 words with lowest and highest idf values.\n",
        "* Can you think of a reason why someone could claim that tf does not have a high impact when processing tweets?\n",
        "* **The output of this excercise** should be a list of words sorted by their idf weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pjNY3H9m5-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa43f53a-23c2-4254-dbd6-2f018204730e"
      },
      "source": [
        "# DF = document frequency df(t), in how many documents (out of all documents) the term t appears\r\n",
        "# IDF = inverse document frequency, m/df(t), where m is the total number of documents in your collection\r\n",
        "DF = {}\r\n",
        "IDF = {}\r\n",
        "import random\r\n",
        "example_token=random.choice(list(token_counter.keys()))\r\n",
        "print(\"Total count for '\", example_token,\"' (selected at random): \", token_counter[example_token])\r\n",
        "print(\"The next part of my code is slow, but it gets there eventually!\")\r\n",
        "for t in token_counter.keys():\r\n",
        "  for s in segmented_tweets:\r\n",
        "      if t in s:\r\n",
        "        if t in DF:\r\n",
        "           DF[t]+=1\r\n",
        "        else: DF[t]=1\r\n",
        "print(\"Document frequency for '\", example_token,\"': \", DF[example_token])\r\n",
        "for t in DF:\r\n",
        "  IDF[t]=len(segmented_tweets)/DF[t]\r\n",
        "print(\"Inverse document frequency for '\", example_token,\"': \", IDF[example_token])  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total count for ' kellahtaminen ' (selected at random):  1\n",
            "The next part of my code is slow, but it gets there eventually!\n",
            "Document frequency for ' kellahtaminen ':  1\n",
            "Inverse document frequency for ' kellahtaminen ':  10000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwxJM9Rdm5-U"
      },
      "source": [
        "## 6) Duplicates or near duplicates\n",
        "\n",
        "* Check whether we have duplicate tweets (in terms of text field only) in our dataset. Duplicate tweet means here that the exactly same tweet text appears more than once in our dataset.\n",
        "* Note: It makes sense to check the duplicates using original tweet texts as the texts were before segmentation. I would also recommend using the full 10,000 dataset here in order to get higher chance of seeing duplicates (this does not require heavy computing).\n",
        "* Try to check whether tweets have additional near-duplicates. Near duplicate means here that tweet text is almost the same in two or more tweets. Ponder what kind of near duplicates there could be and how to find those. Start by considering for example different normalization techniques. Implement some of the techniques you considered.\n",
        "* **The outcome of this exercise** should be a number of unique tweets in our dataset (with possibly counting also which are the most common duplicates) as well as the number of unique tweets after removing also near duplicates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jt0sZrSm5-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb807d02-53f8-4494-d139-a6ee43e9d1ee"
      },
      "source": [
        "dupe_counter = Counter()\r\n",
        "dupe_counter.update(tweetlist)\r\n",
        "print(\"Most common tweets:\", dupe_counter.most_common(20))\r\n",
        "uniquetweets=[]\r\n",
        "for t in dupe_counter:\r\n",
        "  if dupe_counter[t]==1: uniquetweets.append(t)\r\n",
        "print(\"Number of unique tweets: \", len(uniquetweets))\r\n",
        "print(\"Random example: \", uniquetweets[randint(0,len(uniquetweets))])\r\n",
        "# The duplicates seem to be mostly retweets, so I'm thinking near duplicates might be retweets with something added\r\n",
        "# - though I've ever tweeted myself so I'm not sure...\r\n",
        "neardupe_counter = {}\r\n",
        "for t in uniquetweets:\r\n",
        "  for s in uniquetweets:\r\n",
        "    if t in s:\r\n",
        "      if t in neardupe_counter: neardupe_counter[t]+=1\r\n",
        "      else:neardupe_counter[t]=1\r\n",
        "print(\"For some reason the next part appears only after a short delay.\")\r\n",
        "superuniquetweets=[]\r\n",
        "for t in neardupe_counter:\r\n",
        "  if neardupe_counter[t]==1: superuniquetweets.append(t)\r\n",
        "print(\"Number of super unique tweets: \", len(superuniquetweets))\r\n",
        "print(\"Random example: \", superuniquetweets[randint(0,len(superuniquetweets))])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common tweets: [('RT @SitaSalminen: Testasin huvikseen millaisen reaktion lempeys saa aikaan. Suu loksahti auki https://t.co/L7RR70QsZo', 9), ('RT @KeyisQueen: Sosa babyy https://t.co/raoAJv8auH', 8), ('RT @babaBC: T’challa jata hoon kisi ki dhun mein https://t.co/oRkEoGg15B', 8), ('RT @AestheticsJapan: Guiding Light | by Julius Kähkönen\\n(https://t.co/vZMCQYy8Rt) https://t.co/XbnOyg8l7w', 8), ('RT @alvaleryae: Que hermoooosuuraa https://t.co/Tvha0K9GQC', 5), ('Tiedoksi:\\nHelsingin puistoissa ja yleisillä alueilla saa liikkua myös \"ilman järkevän tuntuista syytä\". \\n\\nTuollainen päämäärätön harhailu voi itse asiassa olla ihan rentouttavaa. #Helsinki https://t.co/8PZepPcBF1', 4), ('RT @VartiainenPasi: Näyttäkää minulle ennustaja, joka tiesi tulevaksi nämä otsikot vuosi sitten. https://t.co/apGqjDFRWY', 4), ('Mood ku CV:s ei lue mitään muuta ku nimi, yhteystiedot sekä \"Puhe- ja kirjoitustaito\" ja työpaikkojen ovet vaan pamahtelee auki https://t.co/sVodd2PM5A', 4), ('RT @qualityjaehyun: Jaehyun locksreens💌 https://t.co/TompDxLokd', 3), ('Uskomaton kasvutarina! ☀️\\n\\nTänään oli niin aurinkoista, että hukkasin yhden mulle heitetyn juustonpalan valoisaan keittiönnurkkaan. Etsin ahkerasti, kunnes lopulta löysin tuon jo kadonneeksi luulemani juustonpalan. \\n\\nMun viesti on selkeä: \\nUskokaa unelmiinne. \\nNe voi toteutua. https://t.co/KSVTLZDnL0', 3), ('ABC on edelleen luotetuin huoltoasema - suuri kiitos asiakkaat ja teitä ilolla palvelevat ABC-ammattilaiset ympäri Suomen 🙏 #luotetuinmerkki https://t.co/uyzcD8iQKP', 3), ('RT @lalpra: Sarkara Badalisi, BJP Gellisi https://t.co/CxN08L9NXF', 3), ('HYVÄ @saaraaalto MONSTERS ON MUN TÄN HETKEN LEMPIBIISI NYT TÄÄ TWIITTI JAKOON NIIN ETTÄ TWITTER RÄJÄHTÄÄ JA SAARA VOITTAA JUMALAUTA!!! SAARA! SAARA! SAARA! 🇫🇮❤️🌟🇫🇮❤️🌟🇫🇮❤️🌟🇫🇮❤️🌟🇫🇮❤️🌟🇫🇮❤️🌟🇫🇮❤️🌟🇫🇮❤️🌟🇫🇮❤️🌟🇫🇮❤️🌟 #euroviisut', 3), ('RT @sarasalomaa: \"Kaikilla meillä on rajoitteita.\"\\nTämä lause jää elämään. Sanat köyhälle, sairaalle, työkyvyttömälle ihmiselle. \\n#yleastud…', 3), ('\"Jos sotalapsiin olisi sovellettu nykyisiä lakeja ja käytäntöjä, yksikään heistä ei olisi palannut Suomeen, koska perhesiteen olisi katsottu katkenneen\", sanoo Anita Novitsky @Vaestoliitto. Siinäpä pohdittavaa kansainvälisenä perheiden päivänä. #oikeusperheelämään', 3), ('U17-tytöt etenivät EM-välieriin, BOOOOOM 💥💥 Joukkue kaatoi lohkovaiheen päätöspelissä Hollannin 2-1 ja kohtaa seuraavaksi Espanjan. Suomen maalit tänään Kaisa Juvonen ja Aino Vuorinen 🙌🏼 OI SUOMI ON 🇫🇮 #Pikkuhelmarit #WU17EURO #NEDFIN https://t.co/bfh5AU50Qf', 3), ('.@jyrihakamies korosti @Elinkeinoelama’n kevätseminaarissa, että olennaista on huolehtia Suomen menestyksestä ja houkuttelevuuden lisäämisestä  #kaupungistuminen #stadilande #osaaminen #kaavoitus #liikenne #asuminen https://t.co/W13wwhSIxx', 3), ('RT @topielomaa: Kevään pörisevät pistiäiset haltuun. Muista nämä ennen kuin teet hätiköityjä liikkeitä. #kevät #kevätseuranta https://t.co/…', 3), ('Tiesitkö sinä, että eduskunta tänään hyväksyi CETA-sopimuksen, tai että hallituksen tajuton sote-aikataulu on johtanut siihen, että sosiaali- ja terveysvaliokunta joutuu jättämään käsittelemättä muita tärkeitä asioita? Sen sijaan olet ehkä nähnyt tiedotustilaisuuden... logosta.', 3), ('RT @VilleNiinisto: Kaikkien päättäjien on kyettävä tunnistamaan poliittisen väkivallan vaarallinen erityisluonne: tavoite kumota demokratia…', 2)]\n",
            "Number of unique tweets:  9579\n",
            "Random example:  @paivi65 Huoment ☕🎶🐦\n",
            "Kimito +12   🌞   tuulee kohtalaisesti.\n",
            "For some reason the next part appears only after a short delay.\n",
            "Number of super unique tweets:  9573\n",
            "Random example:  RT @HanneleVestola: Plagointi, peilaaminen, mistä näitä nyt tietää. Ehkä kopioin, ehkä en, sehän ei perussuomalaisessa maailmassa ole niin…\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqR2YGRBFddJ"
      },
      "source": [
        "Comments:\r\n",
        "I'm very new to python, but I managed to get by with liberal copy-pasting and some googling."
      ]
    }
  ]
}